{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5713a95f",
   "metadata": {},
   "source": [
    "   # Axion-like particles analysis of the NGC1275 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa10eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python modules ###\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('xtick', labelsize=20)   \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif',size=25)\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import astropy.units as u\n",
    "from pathlib import Path\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord, Angle\n",
    "from regions import CircleSkyRegion, PointSkyRegion\n",
    "from scipy.stats import chi2\n",
    "import scipy.special as scipys\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import gamma\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb43428",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gammapy modules ###\n",
    "\n",
    "from gammapy.modeling import Fit\n",
    "import gammapy.irf as irf\n",
    "from gammapy.irf import load_cta_irfs\n",
    "from gammapy.data import Observation, Observations, DataStore\n",
    "from gammapy.utils.random import get_random_state\n",
    "from gammapy.maps import MapAxis\n",
    "\n",
    "# models modules\n",
    "from gammapy.modeling.models import (\n",
    "    Model,\n",
    "    Models,\n",
    "    SkyModel,\n",
    "    PowerLawSpectralModel,\n",
    "    PowerLawNormSpectralModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    PointSpatialModel,\n",
    "    GaussianSpatialModel,\n",
    "    TemplateSpatialModel,\n",
    "    FoVBackgroundModel,\n",
    "    SpectralModel,\n",
    "    #Parameter, \n",
    "    TemplateSpectralModel\n",
    ")\n",
    "# dataset modules\n",
    "from gammapy.datasets import (\n",
    "    MapDataset, \n",
    "    MapDatasetOnOff, \n",
    "    MapDatasetEventSampler,\n",
    "    SpectrumDatasetOnOff,\n",
    "    SpectrumDataset, \n",
    "    Datasets,\n",
    "    FluxPointsDataset\n",
    ")\n",
    "\n",
    "from gammapy.maps import MapAxis, WcsGeom, Map, RegionGeom\n",
    "from gammapy.makers import MapDatasetMaker, SpectrumDatasetMaker, ReflectedRegionsBackgroundMaker, WobbleRegionsFinder\n",
    "from gammapy.estimators import FluxPointsEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALPs modules ###\n",
    "\n",
    "from ALPgrid import ALPgrid, previous_limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e3168",
   "metadata": {},
   "source": [
    "## Import the fits files, IACT data of the NGC1275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_flare = Observations()\n",
    "for filename in glob.glob(f\"../../arqus/data/ngc1275/magic/*fits\"):\n",
    "    observations_flare.append(Observation.read(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248387ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations_flare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f7221",
   "metadata": {},
   "source": [
    "### Load in the spectral models that we will be using (PWL & EPWL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eaf3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epwl = ExpCutoffPowerLawSpectralModel()\n",
    "            \n",
    "epwl.amplitude.min = 0\n",
    "epwl.index.min = 0\n",
    "epwl.lambda_.min = 0\n",
    "epwl.reference.min = 0.3\n",
    "    \n",
    "model_epwl = SkyModel(spectral_model=epwl, name=\"EPWL\")\n",
    "#print(model_epwl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwl = PowerLawSpectralModel()\n",
    "\n",
    "pwl.amplitude.min = 0\n",
    "pwl.index.min = 0\n",
    "pwl.reference.min = 0.3 \n",
    "\n",
    "model_pwl = SkyModel(spectral_model=pwl, name=\"PWL\")\n",
    "#print(model_pwl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e83a0a",
   "metadata": {},
   "source": [
    "#### In order to load in the IACT dataset, we need to define the sky region in which the source is located, it's geometry and number of off regions to be used for estimating the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe931d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_coordinates = SkyCoord.from_name(\"NGC1275\")\n",
    "on_center = PointSkyRegion(source_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd3328",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_off_regions = 3\n",
    "region_finder = WobbleRegionsFinder(n_off_regions=n_off_regions)\n",
    "bkg_maker = ReflectedRegionsBackgroundMaker(region_finder=region_finder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ffdc3",
   "metadata": {},
   "source": [
    "## Create a dataset using our observations and definded geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8854442",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_flare = Datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7647842",
   "metadata": {},
   "source": [
    "### Define the energy axes of the dataset for producing the spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENERGY AXES\n",
    "emin             = 80*u.GeV\n",
    "emax             = 2.1*u.TeV\n",
    "en_edges         = np.logspace(  np.log10( emin/u.GeV).value  , np.log10(emax/u.GeV).value,27) * u.GeV \n",
    "energy_axis      = MapAxis.from_edges(en_edges, interp='log' , unit=\"GeV\", name=\"energy\")\n",
    "energy_true_axis = MapAxis.from_energy_bounds(5, 5e4, nbin=40, per_decade=False, unit=\"GeV\", name=\"energy_true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cff41",
   "metadata": {},
   "source": [
    "### Define and fill the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMPTY DATASET AND DATASET MAKER\n",
    "geom = RegionGeom.create(region=on_center, axes=[energy_axis])\n",
    "    \n",
    "# spectrum dataset and its maker (fills the ON counts)\n",
    "dataset_empty = SpectrumDataset.create(\n",
    "    geom=geom, energy_axis_true=energy_true_axis\n",
    ")\n",
    "dataset_maker = SpectrumDatasetMaker(\n",
    "    containment_correction=False, selection=[\"counts\", \"exposure\", \"edisp\"]\n",
    ")\n",
    "\n",
    "\n",
    "for obs in observations_flare:\n",
    "    dataset = dataset_maker.run(dataset_empty.copy(name=f\"{obs.obs_id}\"), obs)\n",
    "    dataset.mask_fit = dataset.counts.geom.energy_mask(emin, emax)\n",
    "    dataset_on_off = bkg_maker.run(dataset, obs)\n",
    "    dataset_on_off.mask_fit = dataset_on_off.counts.geom.energy_mask(emin, emax)\n",
    "    datasets_flare.append(dataset_on_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b41ce",
   "metadata": {},
   "source": [
    "### Stack the observations for easier handling the data and fitting the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_flare_epwl        = datasets_flare.stack_reduce(name=\"NGC1275\")\n",
    "epwl2                 = epwl.copy()\n",
    "epwl2.tag[0]          ='ExpCutoffPowerLawSpectralModel'\n",
    "model                 = SkyModel(spectral_model=epwl2, name=\"NGC1275\")\n",
    "datasets_flare_epwl.models = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc474035",
   "metadata": {},
   "source": [
    "### Fit the dataset to the smooth function (model that we defined) and print the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8291c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit           = Fit()\n",
    "results_flare_epwl = fit.run(datasets_flare_epwl)\n",
    "\n",
    "print(datasets_flare_epwl)\n",
    "print(results_flare_epwl.parameters.to_table())\n",
    "datasets_flare_epwl.plot_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251ae561",
   "metadata": {},
   "source": [
    "We are going to save this best fit model in order to plot it later in comparison to the best ALPs model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit_model = model.copy()\n",
    "model_out_dir = f\"\"\n",
    "Path(model_out_dir).mkdir(exist_ok=True, parents=True)\n",
    "Models(best_fit_model).write(\n",
    "    f\"{model_out_dir}/best_fit.yaml\", write_covariance=True, overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca672c6",
   "metadata": {},
   "source": [
    "### Extract the fluxpoints of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "e_min, e_max = 0.08, 2.1\n",
    "energy_edges = np.geomspace(e_min, e_max, 21) * u.TeV\n",
    "\n",
    "fpe = FluxPointsEstimator(\n",
    "    energy_edges=energy_edges, source=\"NGC1275\", selection_optional=\"all\"\n",
    ")\n",
    "flux_points = fpe.run(datasets=datasets_flare_epwl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points_dataset = FluxPointsDataset(\n",
    "    data=flux_points, models=datasets_flare_epwl.models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "ax = flux_points.plot(sed_type=\"e2dnde\", color=\"darkorange\")\n",
    "flux_points.plot_ts_profiles(ax=ax, sed_type=r\"e2dnde\")\n",
    "emin, emax = 80, 2100#5e-2,0.3e1\n",
    "ymin, ymax = 4e-12,3e-10\n",
    "ax.set_xlim([emin, emax])\n",
    "ax.set_ylim([ymin, ymax])\n",
    "ax.set_ylabel(r\"$E^2 dN/dE \\quad  [ erg \\; cm^{-2} s^{-1}] $\",size=20)\n",
    "for e in dataset_on_off.counts.geom.axes[\"energy\"].edges:\n",
    "    ax.vlines( e,ymin,ymax, color=\"black\", alpha=0.1 ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5c952",
   "metadata": {},
   "source": [
    "#### Exercise: Repeat the fit of the dataset using the power-law function, and extract the fluxpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./Solutions/pwl_fit.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87a560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57da935d",
   "metadata": {},
   "source": [
    "## ALPs analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8548cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#We define an empty list which will contain the -2logL (i.e. stat_sum() ) \n",
    "list_logL_per_gridpoint_ngc1275 = []\n",
    "# for all ALP-grid points and for all magnetic field realization:\n",
    "# Example:\n",
    "# list_logL_per_gridpoint[k][m,g]\n",
    "# is the -2logL for the k-th magnetic field realization \n",
    "# and for the ALP model with mass m and coupling g\n",
    "\n",
    "dict_datasets = { }\n",
    "path_name = \"./Pyy_models/Models\" \n",
    "for name in glob.glob(path_name+\"/*.npy\"):  \n",
    "\n",
    "    ## GET INFO ON M AND G FROM FILE NAME\n",
    "    names_split = str.split(name, \"_\")\n",
    "    g = names_split[-2]\n",
    "    m = names_split[-4]\n",
    "    g = float(g) * 1e-14 / u.GeV #these are just to normalize the names of the files given by the parameters of the models\n",
    "    m = float(m) * 1e-2 * u.neV\n",
    "\n",
    "    en_absorp_array     = np.load(name)\n",
    "    energy              = en_absorp_array[0] * u.GeV\n",
    "    values              = en_absorp_array[1+25] * u.Unit(\"\") # whatever numer from 1 to 100\n",
    "    absorption          = TemplateSpectralModel(energy, values)\n",
    "    spectral_model      = absorption * epwl\n",
    "    model               = SkyModel(spectral_model=spectral_model, name=\"NGC1275\")\n",
    "    new_datasets        = datasets_flare_epwl.copy()\n",
    "    new_datasets.models = model\n",
    "    dict_datasets[g,m]  = new_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "alp = ALPgrid(dict_datasets)\n",
    "# we plot the grid of points we are considering\n",
    "fig, ax = alp.plot_grid()\n",
    "# optional, add previous limits\n",
    "previous_limits(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee3d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this might take some time since the function \n",
    "# get_TS_per_gridpoint() will run the fit class on all datasets\n",
    "# saved in the ALPgrid class\n",
    "alp.get_TS_per_gridpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the Wilks theorem a CL of 95 or 99 % is reached for TS 6.0 and 9.2\n",
    "contour_dict = dict( colors=[\"black\"] , levels=[6.0, 9.2], linewidths=[4,3,2,1], linestyles=\"--\" ) \n",
    "# OR WITH PREVIOUS LIMITS AS COMPARISON\n",
    "fig, ax = alp.plot_grid(show='TS',contour=True,lines=True,**contour_dict)\n",
    "#ax.set_ylim( [ 14.73612599e-11,5e-10] )\n",
    "ax.set_ylim( [ 4.5e-13,5e-10] )\n",
    "#ax.set_ylim( [ 4.5e-13,9.5e-11] )\n",
    "ax.set_xlim( [ 9e-10,1.2e-6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set alp_index to the one that maximize the likelihood (best fit) ....\n",
    "alp_index = np.argmin( list( alp.TS_per_gridpoint.values() ) )\n",
    "# .... or to the one that minimize the likelihood (worst fit) ....\n",
    "#alp_index = np.argmax( list( alp.TS_per_gridpoint.values() ) )\n",
    "# ... or choose the one that you prefer\n",
    "#best_index = 0\n",
    "\n",
    "m, g = list(alp.TS_per_gridpoint.keys())[alp_index ]\n",
    "print(m)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da989458",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dataset  = alp.dataset_per_gridpoint[m,g]\n",
    "fit             = Fit()\n",
    "results         = fit.run(chosen_dataset)\n",
    "chosen_dataset.stat_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "malp = chosen_dataset.models['NGC1275']\n",
    "\n",
    "e_min, e_max = 5e-2*u.TeV ,0.3e1*u.TeV\n",
    "\n",
    "energies = np.geomspace(e_min, e_max, 1000) \n",
    "fluxies  = malp.spectral_model(energies)*energies**2\n",
    "fluxies  = fluxies.to( u.Unit(\" erg cm-2 s-1 \") )\n",
    "\n",
    "\n",
    "kwargs_fp    = dict(label=\"DL3 flux points\")\n",
    "kwargs_fp.setdefault(\"label\", \"Flux points\")\n",
    "kwargs_fp.setdefault(\"sed_type\", \"e2dnde\")\n",
    "\n",
    "kwargs_spectrum = dict(\n",
    "    kwargs_model = dict(label=\"Fit on DL3 flux points\"),\n",
    "    kwargs_fp    = dict(label=\"DL3 flux points\")\n",
    ")\n",
    "\n",
    "ax = flux_points_dataset.plot_fit(kwargs_spectrum=kwargs_spectrum)\n",
    "\n",
    "ax[0].plot(energies,fluxies ,c=\"goldenrod\",label=\"Best ALP model\")\n",
    "\n",
    "emin, emax = 5e-2,0.3e1\n",
    "ymin, ymax = 2e-12,3e-10\n",
    "#ax[0].set_xlim([emin, emax])\n",
    "ax[0].set_ylim([ymin, ymax])\n",
    "\n",
    "ax[0].set_ylabel(r\"$E^2 dN/dE \\quad  [ erg \\; cm^{-2} s^{-1}] $\",size=20)\n",
    "ax[1].set_ylabel(r\"$Residuals$\",size=20) #data-model / model\n",
    "#ax.set_ylabel(r\"$E^2 dN/dE \\quad  [ erg \\; cm^{-2} s^{-1}] $\",size=10)\n",
    "#for e in dataset_on_off.counts.geom.axes[\"energy\"].edges:\n",
    "#    ax.vlines( e,ymin,ymax, color=\"black\", alpha=0.1 ) \n",
    "#ax.legend(fontsize=10)\n",
    "ax[0].legend(fontsize=10)\n",
    "plt.subplots_adjust(hspace=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5ced0",
   "metadata": {},
   "source": [
    "## How to set the contrains on the ALPs paramter space? (remember the Wilks' theorem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe68d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./solutions/W_theorem.py\n",
    "Write the Wilks' theorem here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a574add",
   "metadata": {},
   "source": [
    "#### We can load in the pre-calculated log likelihood values to check the distirbution of the test statistics and decide the way to constrain the ALPs space. In this case, we will use the TS of the null hypothesis and compare it to a TS distribution of the alternative hypothesis coming from 100 simulations of the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195efe46",
   "metadata": {},
   "source": [
    "#### Null-hypothesis fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69893132",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder=(\"./Stats/ts_list_sim_null.p\")\n",
    "with open(f'{path_to_folder}', 'rb') as fp:\n",
    "    TS_list_sim = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Solutions/TS_null.py\n",
    "sets = list(range(0,100))\n",
    "#sets = list(range(0,70))\n",
    "TS_list_sim = []\n",
    "for arg in sets:\n",
    "    # WE SAVE THE NULL HYPOTESIS logL\n",
    "    logL_null = list_logL_null[arg]\n",
    "    # WE LOAD THE SIMULATION\n",
    "    path_to_folder=(\"./Stats/List_per_simulation/list_logL_per_gridpoint_model_simulation_{}.p\".format(arg+1))\n",
    "    with open(f'{path_to_folder}', 'rb') as fp:\n",
    "        list_logL_arg = pickle.load(fp)\n",
    "        # SAVE THE TRUE VALUE OF m and g USED FOR THE SIMULATION\n",
    "        mgtrue = list( list_logL_arg[0].keys() )[61]\n",
    "        # FOR EACH ALP GRIDPOINT WE FIND THE 95TH PERCENTILE\n",
    "        # AND WE SAVE ALL THESE VALUES IN dictionary logL_Bfield\n",
    "        logL_Bfield = {}\n",
    "        for mgg in list_logL_arg[0].keys():\n",
    "            listLogL_perBfield = [list_logL_arg[k][mgg] for k in range(100)]\n",
    "            percentile         = 5\n",
    "            logL               =  np.sort( listLogL_perBfield)[percentile]\n",
    "            logL_Bfield[mgg]   = logL\n",
    "        # WE CONVERT THE DICT TO A LIST\n",
    "        list_logL_Bfield =  [logL_Bfield[key] for key in logL_Bfield.keys()]\n",
    "        # WE ADD THE NULL HYPOTESIS logL\n",
    "        list_logL_Bfield.append(logL_null)\n",
    "        # WE LOOK FOR THE MINIMUM VALUE\n",
    "        logL_min  = np.min(list_logL_Bfield )\n",
    "        # WE ARE FINALLY READY FOR COMPUTING THE TS FOR THE arg-th SIMULATION\n",
    "        # IF DATA SIMULATED WITH TRUE HYPOTHESIS DO THIS\n",
    "        logL_True_Hypot = logL_null\n",
    "        # IF DATA SIMULATED WITH HYPOTHESIS mgtrue\n",
    "        #logL_True_Hypot  =  logL_Bfield[mgtrue]\n",
    "        TS  =  logL_True_Hypot -  logL_min\n",
    "        TS_list_sim.append(TS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a665ee6",
   "metadata": {},
   "source": [
    "#### Plot the distribution of the TS  from the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78eb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46948f4b",
   "metadata": {},
   "source": [
    "#### Along with the chi^2 function, we want to plot it's generalisation, gamma function, in order to plot it with our TS distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_func(x, alpha, beta):\n",
    "    return gamma.cdf(x, a=alpha , scale=1/beta )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7954fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10),nrows=1, ncols=1)\n",
    "\n",
    "\n",
    "DOF = 2 # Number of free parameters that have been fitted\n",
    "\n",
    "min_TS      = 0 #np.min(TS_list)\n",
    "max_TS      = np.max(TS_list_sim)\n",
    "TS_bins     = np.linspace(min_TS ,max_TS,1500)\n",
    "TS_CDF      = np.array([np.sum( TS_list_sim  < i ) for i in TS_bins])/len(TS_list_sim)\n",
    "ax.plot(TS_bins, TS_CDF , color='blue', label=\"Null hypothesis CDF\")\n",
    "\n",
    "popt, pcov = curve_fit(gamma_func, TS_bins, TS_CDF)\n",
    "fun1 = gamma_func(TS_bins, popt[0], popt[1])\n",
    "ax.plot(TS_bins, fun1, alpha=0.3, linewidth=4, label=r\"$\\Gamma$ function\")\n",
    "\n",
    "Chi2_CDF = chi2.cdf(TS_bins,df=DOF )\n",
    "ax.plot(TS_bins,Chi2_CDF, color='black',alpha=0.3, linewidth=4, label=r\"CDF of $\\chi^2_{df = \"+str(DOF) +\"}$\")\n",
    "ax.set_ylabel(\"CDF\")\n",
    "ax.set_xlabel(\"Statistic\")\n",
    "ax.set_title(\"CDF of the Statistic for the Binned case - null hypothesis\")\n",
    "ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc25c5",
   "metadata": {},
   "source": [
    "#### Let's do the same for the alternative hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder=(\"./Stats/ts_list_sim_alt.p\")\n",
    "with open(f'{path_to_folder}', 'rb') as fp:\n",
    "    TS_list_alt = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622803e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./Solutions/TS_alt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e1ba7",
   "metadata": {},
   "source": [
    "#### Plot the distribution of the TS  from the alternative hypothesis, along with the chi^2 and the TS distribution of the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c15be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))#,nrows=1, ncols=1)\n",
    "\n",
    "\n",
    "DOF = 2 # Number of free parameters that have been fitted\n",
    "\n",
    "min_TS      = 0 #np.min(TS_list)\n",
    "max_TS      = 30#np.max(TS_list_sim)\n",
    "TS_bins     = np.linspace(min_TS ,max_TS,1500)\n",
    "TS_CDF      = np.array([np.sum( TS_list_sim  < i ) for i in TS_bins])/len(TS_list_sim)\n",
    "ax.plot(TS_bins, TS_CDF , color='blue', label=\"CDF - null hypothesis\")\n",
    "\n",
    "#Chi2_CDF = chi2.cdf(TS_bins,df=DOF )\n",
    "\n",
    "popt, pcov = curve_fit(gamma_func, TS_bins, TS_CDF)\n",
    "fun1 = gamma_func(TS_bins, popt[0], popt[1])\n",
    "ax.plot(TS_bins, fun1, alpha=0.3, linewidth=4, label=r\"$\\Gamma$(2.72,\\;0.74)\")\n",
    "\n",
    "\n",
    "min_TS_alt      = 0 #np.min(TS_list)\n",
    "max_TS_alt     = 30#np.max(TS_list_alt)\n",
    "TS_bins_alt     = np.linspace(min_TS_alt ,max_TS_alt,1500)\n",
    "TS_CDF_alt      = np.array([np.sum( TS_list_alt  < i ) for i in TS_bins_alt_1])/len(TS_list_alt)\n",
    "ax.plot(TS_bins_alt_1, TS_CDF_alt , color='green', label=r\"CDF - alternative hypothesis ($m_a=215.44\\;\\mathrm{neV},g_{a\\gamma}=8\\times10^{-11}\\mathrm{GeV}^{-1}$)\")\n",
    "Chi2_CDF_alt = chi2.cdf(TS_bins_alt,df=DOF )\n",
    "\n",
    "popt_alt, pcov_alt = curve_fit(gamma_func, TS_bins_alt, TS_CDF_alt)\n",
    "fun2 = gamma_func(TS_bins_alt, popt_alt[0], popt_alt[1])\n",
    "ax.plot(TS_bins_alt, fun2, alpha=0.3, linewidth=4, color='green',label=r\"$\\Gamma$(2.45,\\;0.25)\")\n",
    "\n",
    "plt.plot(TS_bins_alt,Chi2_CDF_alt, color='black',alpha=0.3, linewidth=4, label=r\"CDF of $\\chi^2_{df = \"+str(DOF) +\"}$\")\n",
    "plt.legend(loc=\"lower right\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8b3ce",
   "metadata": {},
   "source": [
    "Now, let's find the effective number of d.o.f. for which the gamma function is fitting our TS distributions and giving us 95% and 99% confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./Solutions/null_fit_95.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a24d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./Solutions/null_fit_99.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./Solutions/alt_fit_95.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b691462",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ./Solutions/alt_fit_99.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
